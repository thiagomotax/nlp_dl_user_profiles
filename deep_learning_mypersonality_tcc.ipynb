{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep learning mypersonality_tcc",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kIhY6Nsn_jhnl5l78f7kokGbYiyzuzh0",
      "authorship_tag": "ABX9TyPsBP80TiA4Z1F4XARwjY+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagomotax/nlp_dl_user_profiles/blob/main/deep_learning_mypersonality_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fYFX0edwixu"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z21PAHJAgALN"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwcQJDWIjomP"
      },
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/tcc/datasets/mypersonality/mypersonality_final.csv', encoding=\"ISO-8859-1\")\n",
        "df = df.drop(['#AUTHID',  'sEXT', 'sNEU', 'sAGR', 'sCON', 'sOPN', 'DATE', 'NETWORKSIZE', 'BETWEENNESS', 'NBETWEENNESS','DENSITY','BROKERAGE','NBROKERAGE','TRANSITIVITY'], axis = 1) \n",
        "df.cNEU.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cEXT.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cAGR.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cCON.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)\n",
        "df.cOPN.replace(to_replace=['n', 'y'], value=[0, 1], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c2Oo9MILTCu"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56VRHT9-Nabh"
      },
      "source": [
        "#pre-processing\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "newStopWords = ['propname', 'im', 'propnames' '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "stop.extend(newStopWords)\n",
        "\n",
        "\n",
        "df['STATUS'] = df['STATUS'].str.lower() #lowercase\n",
        "df['STATUS'] = df['STATUS'].str.replace('[{}]'.format(string.punctuation), '') #ponctuaction\n",
        "df['STATUS'] = df['STATUS'].str.replace(r'\\d+','') #numbers\n",
        "df['STATUS'] = df['STATUS'].str.replace(' +', ' ') #this should replace all multiple spaces with a single space\n",
        "df['STATUS'] = df['STATUS'].str.strip() #remove all spaces from the start and end\n",
        "df['STATUS'] = df['STATUS'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop])) #stopwords\n",
        "\n",
        "# df['STATUS'] = [x.replace(\"propnames\", \"\") for x in df['STATUS']]\n",
        "# df['STATUS'] = [x.replace(\"propname\", \"\") for x in df['STATUS']]\n",
        "\n",
        "df.replace(\"\", np.nan, inplace=True) #empty lines to nan\n",
        "df.dropna(how='any', inplace=True) #remove nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNiRWXpdM8Ai"
      },
      "source": [
        "#prepare EDA input to proccess in local computer (due to incompatibilities of the library with google collab)\n",
        "label_cols = [\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "df['one_hot_labels'] = df['cEXT'].map(str) + df['cNEU'].map(str) + df['cAGR'].map(str) + df['cCON'].map(str) + df['cOPN'].map(str)\n",
        "df = df.drop([\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"], axis=1)\n",
        "frame = df[['one_hot_labels', 'STATUS']]\n",
        "frame.to_csv('input_mypersonalityEDA.txt', header=False, index=False, sep='\\t', mode='a') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYzHQxpoOBvA"
      },
      "source": [
        "#receives input processed by EDA (output)\n",
        "import re\n",
        "new_df = pd.read_csv('/content/output_mypersonalityEDA.txt', names=['content'], sep=\"\\f\", header=None)\n",
        "rows_list = []\n",
        "for index, row in new_df.iterrows():\n",
        "  props = re.split(r'\\t+', row['content'])\n",
        "  cEXT = props[0][0]\n",
        "  cNEU = props[0][1]\n",
        "  cAGR = props[0][2]\n",
        "  cCON = props[0][3]\n",
        "  cOPN = props[0][4]\n",
        "  STATUS = props[1]\n",
        "  dataTemp = {}\n",
        "  dataTemp.update({'cEXT':cEXT, 'cNEU':cNEU, 'cAGR':cAGR, 'cCON':cCON, 'cOPN':cOPN, 'STATUS':STATUS})\n",
        "  rows_list.append(dataTemp)\n",
        "\n",
        "df = pd.DataFrame(rows_list, columns=[\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\", \"STATUS\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiVtb4XIn6cB"
      },
      "source": [
        "df.STATUS = df.STATUS.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsqG6Upc5rO3"
      },
      "source": [
        "df.STATUS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLqNvAlkstek"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ-gbCwNmA4k"
      },
      "source": [
        "#split training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "list_classes = [\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.15, shuffle=True)\n",
        "\n",
        "list_sentences_train = train.STATUS\n",
        "list_sentences_test = test.STATUS\n",
        "\n",
        "y_train = train[list_classes].values\n",
        "y_test = test[list_classes].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFlMnREAmNdB"
      },
      "source": [
        "#tokenize data\n",
        "max_features = 5000 \n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features, lower=True, filters='') #Only the most common num_words-1 will kept, by default, all punctuation is removed, turning the texts into space-separated sequences of words\n",
        "\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_EqK-NqmQfu"
      },
      "source": [
        "# pad data\n",
        "maxlen = 20\n",
        "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqb7Poc-ky0w"
      },
      "source": [
        "import numpy as np\r\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr2uAaaeky3W"
      },
      "source": [
        "#function to load and convert and proccess dataset to Glove, Word2vec or FastText requirements\r\n",
        "def loadEmbeddingMatrix(typeToLoad):\r\n",
        "        #load different embedding fil depending on which embedding matrix are going to experiment with\r\n",
        "        if(typeToLoad==\"glove\"):\r\n",
        "            EMBEDDING_FILE='/content/drive/MyDrive/tcc/glove.6B.200d.txt'\r\n",
        "            embed_size = 200\r\n",
        "        elif(typeToLoad==\"word2vec\"):\r\n",
        "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True)\r\n",
        "            embed_size = 300\r\n",
        "        elif(typeToLoad==\"fasttext\"):\r\n",
        "            EMBEDDING_FILE='../input/fasttext/wiki.simple.vec'\r\n",
        "            embed_size = 300\r\n",
        "\r\n",
        "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\r\n",
        "            embeddings_index = dict()\r\n",
        "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\r\n",
        "            f = open(EMBEDDING_FILE)\r\n",
        "            for line in f:\r\n",
        "                #split up line into an indexed array\r\n",
        "                values = line.split()\r\n",
        "                #first index is word\r\n",
        "                word = values[0]\r\n",
        "                #store the rest of the values in the array as a new array\r\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "                embeddings_index[word] = coefs #50 dimensions\r\n",
        "            f.close()\r\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\r\n",
        "        else:\r\n",
        "            embeddings_index = dict()\r\n",
        "            for word in word2vecDict.wv.vocab:\r\n",
        "                embeddings_index[word] = word2vecDict.word_vec(word)\r\n",
        "            print('Loaded %s word vectors.' % len(embeddings_index))\r\n",
        "            \r\n",
        "        gc.collect()\r\n",
        "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \r\n",
        "        #same statistics for the rest of our own random generated weights. \r\n",
        "        all_embs = np.stack(list(embeddings_index.values()))\r\n",
        "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\r\n",
        "        \r\n",
        "        nb_words = len(tokenizer.word_index)\r\n",
        "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\r\n",
        "        #the size will be Number of Words in Vocab X Embedding Size\r\n",
        "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\r\n",
        "        gc.collect()\r\n",
        "\r\n",
        "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \r\n",
        "        #our own dictionary and loaded pretrained embedding. \r\n",
        "        embeddedCount = 0\r\n",
        "        for word, i in tokenizer.word_index.items():\r\n",
        "            i-=1\r\n",
        "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\r\n",
        "            embedding_vector = embeddings_index.get(word)\r\n",
        "            #and store inside the embedding matrix that we will train later on.\r\n",
        "            if embedding_vector is not None: \r\n",
        "                embedding_matrix[i] = embedding_vector\r\n",
        "                embeddedCount+=1\r\n",
        "        print('total embedded:',embeddedCount,'common words')\r\n",
        "        \r\n",
        "        del(embeddings_index)\r\n",
        "        gc.collect()\r\n",
        "        \r\n",
        "        #finally, return the embedding matrix\r\n",
        "        return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te_lKzLtky6N"
      },
      "source": [
        "embedding_matrix = loadEmbeddingMatrix('glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-1WHt7fky8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8933b07-36e6-48e3-df71-8b90438d57ee"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19597, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p61swCR6mVty"
      },
      "source": [
        "#We begin our defining an input layer \n",
        "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlXR-ZOpne1_"
      },
      "source": [
        "#embed layer with glove\n",
        "\n",
        "#default comented\n",
        "#embed_size = 128\n",
        "#x = Embedding(max_features, embed_size)(inp)\n",
        "\n",
        "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhz3QiLkrNhG"
      },
      "source": [
        "#base hidden and output layers\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#default\n",
        "#x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
        "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "\n",
        "x = GlobalMaxPool1D()(x)\n",
        "\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# - 1 ou +1 (4)\n",
        "x = Dense(5, activation=\"sigmoid\")(x)\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.AUC(), tf.keras.metrics.Recall() ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "redGqN3MrP1g"
      },
      "source": [
        "#training and validation parameters\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "#ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "batch_size = 32 #refers to the number of training examples used in an iteration.\n",
        "epochs = 500\n",
        "history = model.fit(\n",
        "    X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.10\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbuV2gNCs_6O"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RGNUviM2o0J"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o94yEZPTizyp"
      },
      "source": [
        "#word count tokenizer (useful to define ANN parameters)\r\n",
        "x = sorted((tokenizer.word_index).items(), key=lambda x: x[1], reverse=True)\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap89kR0mbSzF"
      },
      "source": [
        "#plot\r\n",
        "counts = []\r\n",
        "df_status = df.drop(['STATUS'], axis=1)\r\n",
        "categories = list(df_status.columns.values)\r\n",
        "for i in categories:\r\n",
        "    counts.append((i, df_status[i].sum()))\r\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\r\n",
        "\r\n",
        "df_stats.plot(x='category', y='number_of_comments', kind='bar', legend=False, grid=True, figsize=(8, 5))\r\n",
        "plt.title(\"Número de atualizações de status em cada fator\")\r\n",
        "plt.ylabel('# ocorrências', fontsize=12)\r\n",
        "plt.xlabel('fator', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n2w43dtiQti"
      },
      "source": [
        "#plot\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "rowsums = df.iloc[:,2:].sum(axis=1)\r\n",
        "x=rowsums.value_counts()\r\n",
        "#plot\r\n",
        "plt.figure(figsize=(8,5))\r\n",
        "ax = sns.barplot(x.index, x.values)\r\n",
        "plt.title(\"Número de atualizações de status que possuem multiplos fatores\")\r\n",
        "plt.ylabel('# de ocorrências', fontsize=12)\r\n",
        "plt.xlabel('# de fatores', fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqz0SwmFisOu"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\r\n",
        "\r\n",
        "#model plot\r\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgjOYSALiu7L"
      },
      "source": [
        "#plot\r\n",
        "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\r\n",
        "plt.hist(totalNumWords,bins = np.arange(0,410,10))\r\n",
        "plt.title(\"Número de palavras por sentença\")\r\n",
        "plt.ylabel('# de sentenças', fontsize=12)\r\n",
        "plt.xlabel('# de palavras', fontsize=12)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOzy-1bio0E"
      },
      "source": [
        "#wordcloud\r\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "stopwords = set(STOPWORDS)\r\n",
        "stopwords.update([\"propname\"])\r\n",
        "\r\n",
        "text = \" \".join(review for review in df['STATUS'])\r\n",
        "\r\n",
        "wordcloud = WordCloud(background_color=\"black\",\r\n",
        "                      width=1600, height=800).generate(text)\r\n",
        "\r\n",
        "# Display the generated image:\r\n",
        "# the matplotlib way:\r\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "wordcloud.to_file(\"wordcloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt8cdjvnooBr"
      },
      "source": [
        "#metric plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#auc\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "\n",
        "plt.title('model auc')\n",
        "plt.ylabel('auc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#precision\n",
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "\n",
        "plt.title('model precision')\n",
        "plt.ylabel('precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#recall\n",
        "plt.plot(history.history['recall'])\n",
        "plt.plot(history.history['val_recall'])\n",
        "\n",
        "plt.title('model recall')\n",
        "plt.ylabel('recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wUYnaxVhSRA"
      },
      "source": [
        "#save model for later use\n",
        "from keras.models import load_model\n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2YSiozDixC"
      },
      "source": [
        "#load model \r\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd0TBv7_hVh6"
      },
      "source": [
        "#plot\n",
        "traits_labels = df[[\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"]]\n",
        "\n",
        "\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 10\n",
        "fig_size[1] = 8\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "\n",
        "traits_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IsCBLJ-hWcw"
      },
      "source": [
        "# predictions = model.predict(np.expand_dims(X_test[500], 0))\n",
        "\n",
        "# print(tokenizer.sequences_to_texts([X_test[500]]))\n",
        "# print(y_test[500])\n",
        "# print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}